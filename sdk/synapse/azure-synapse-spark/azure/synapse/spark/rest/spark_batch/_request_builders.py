# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from typing import TYPE_CHECKING

from azure.core.pipeline.transport._base import _format_url_section
from azure.synapse.spark.core.rest import HttpRequest
from msrest import Serializer

if TYPE_CHECKING:
    # pylint: disable=unused-import,ungrouped-imports
    from typing import Any, Optional

_SERIALIZER = Serializer()


def build_get_spark_batch_jobs_request(
    **kwargs  # type: Any
):
    # type: (...) -> HttpRequest
    """List all spark batch jobs which are running under a particular spark pool.

    See https://aka.ms/azsdk/python/llcwiki for how to incorporate this request builder into your
    code flow.

    :keyword from_parameter: Optional param specifying which index the list should begin from.
    :paramtype from_parameter: int
    :keyword size: Optional param specifying the size of the returned list.
                 By default it is 20 and that is the maximum.
    :paramtype size: int
    :keyword detailed: Optional query param specifying whether detailed response is returned beyond
     plain livy.
    :paramtype detailed: bool
    :return: Returns an :class:`~azure.synapse.spark.core.rest.HttpRequest` that you will pass to
     the client's `send_request` method. See https://aka.ms/azsdk/python/protocol/quickstart for how
     to incorporate this response into your code flow.
    :rtype: ~azure.synapse.spark.core.rest.HttpRequest

    Example:
        .. code-block:: python

            # response body for status code(s): 200
            response.json() == {
                "from": "int",
                "sessions": [
                    {
                        "appId": "str (optional)",
                        "appInfo": {
                            "str": "str (optional)"
                        },
                        "artifactId": "str (optional)",
                        "errorInfo": [
                            {
                                "errorCode": "str (optional)",
                                "message": "str (optional)",
                                "source": "str (optional)"
                            }
                        ],
                        "id": "int",
                        "jobType": "str (optional)",
                        "livyInfo": {
                            "currentState": "str (optional)",
                            "deadAt": "datetime (optional)",
                            "jobCreationRequest": {
                                "archives": [
                                    "str (optional)"
                                ],
                                "args": [
                                    "str (optional)"
                                ],
                                "className": "str (optional)",
                                "conf": {
                                    "str": "str (optional)"
                                },
                                "driverCores": "int (optional)",
                                "driverMemory": "str (optional)",
                                "executorCores": "int (optional)",
                                "executorMemory": "str (optional)",
                                "file": "str (optional)",
                                "files": [
                                    "str (optional)"
                                ],
                                "jars": [
                                    "str (optional)"
                                ],
                                "name": "str (optional)",
                                "numExecutors": "int (optional)",
                                "pyFiles": [
                                    "str (optional)"
                                ]
                            },
                            "killedAt": "datetime (optional)",
                            "notStartedAt": "datetime (optional)",
                            "recoveringAt": "datetime (optional)",
                            "runningAt": "datetime (optional)",
                            "startingAt": "datetime (optional)",
                            "successAt": "datetime (optional)"
                        },
                        "log": [
                            "str (optional)"
                        ],
                        "name": "str (optional)",
                        "pluginInfo": {
                            "cleanupStartedAt": "datetime (optional)",
                            "currentState": "str (optional)",
                            "monitoringStartedAt": "datetime (optional)",
                            "preparationStartedAt": "datetime (optional)",
                            "resourceAcquisitionStartedAt": "datetime (optional)",
                            "submissionStartedAt": "datetime (optional)"
                        },
                        "result": "str (optional)",
                        "schedulerInfo": {
                            "cancellationRequestedAt": "datetime (optional)",
                            "currentState": "str (optional)",
                            "endedAt": "datetime (optional)",
                            "scheduledAt": "datetime (optional)",
                            "submittedAt": "datetime (optional)"
                        },
                        "sparkPoolName": "str (optional)",
                        "state": "str (optional)",
                        "submitterId": "str (optional)",
                        "submitterName": "str (optional)",
                        "tags": {
                            "str": "str (optional)"
                        },
                        "workspaceName": "str (optional)"
                    }
                ],
                "total": "int"
            }
    """

    from_parameter = kwargs.pop('from_parameter', None)  # type: Optional[int]
    size = kwargs.pop('size', None)  # type: Optional[int]
    detailed = kwargs.pop('detailed', None)  # type: Optional[bool]

    accept = "application/json"

    # Construct URL
    url = kwargs.pop("template_url", '/batches')

    # Construct parameters
    query_parameters = kwargs.pop("params", {})  # type: Dict[str, Any]
    if from_parameter is not None:
        query_parameters['from'] = _SERIALIZER.query("from_parameter", from_parameter, 'int')
    if size is not None:
        query_parameters['size'] = _SERIALIZER.query("size", size, 'int')
    if detailed is not None:
        query_parameters['detailed'] = _SERIALIZER.query("detailed", detailed, 'bool')

    # Construct headers
    header_parameters = kwargs.pop("headers", {})  # type: Dict[str, Any]
    header_parameters['Accept'] = _SERIALIZER.header("accept", accept, 'str')

    return HttpRequest(
        method="GET",
        url=url,
        params=query_parameters,
        headers=header_parameters,
        **kwargs
    )


def build_create_spark_batch_job_request(
    **kwargs  # type: Any
):
    # type: (...) -> HttpRequest
    """Create new spark batch job.

    See https://aka.ms/azsdk/python/llcwiki for how to incorporate this request builder into your
    code flow.

    :keyword json: Pass in a JSON-serializable object (usually a dictionary). See the template in
     our example to find the input shape. Livy compatible batch job request payload.
    :paramtype json: Any
    :keyword content: Pass in binary content you want in the body of the request (typically bytes,
     a byte iterator, or stream input). Livy compatible batch job request payload.
    :paramtype content: Any
    :keyword detailed: Optional query param specifying whether detailed response is returned beyond
     plain livy.
    :paramtype detailed: bool
    :return: Returns an :class:`~azure.synapse.spark.core.rest.HttpRequest` that you will pass to
     the client's `send_request` method. See https://aka.ms/azsdk/python/protocol/quickstart for how
     to incorporate this response into your code flow.
    :rtype: ~azure.synapse.spark.core.rest.HttpRequest

    Example:
        .. code-block:: python

            # JSON input template you can fill out and use as your `json` input.
            json = {
                "archives": [
                    "str (optional)"
                ],
                "args": [
                    "str (optional)"
                ],
                "artifactId": "str (optional)",
                "className": "str (optional)",
                "conf": {
                    "str": "str (optional)"
                },
                "driverCores": "int (optional)",
                "driverMemory": "str (optional)",
                "executorCores": "int (optional)",
                "executorMemory": "str (optional)",
                "file": "str",
                "files": [
                    "str (optional)"
                ],
                "jars": [
                    "str (optional)"
                ],
                "name": "str",
                "numExecutors": "int (optional)",
                "pyFiles": [
                    "str (optional)"
                ],
                "tags": {
                    "str": "str (optional)"
                }
            }

            # response body for status code(s): 200
            response.json() == {
                "appId": "str (optional)",
                "appInfo": {
                    "str": "str (optional)"
                },
                "artifactId": "str (optional)",
                "errorInfo": [
                    {
                        "errorCode": "str (optional)",
                        "message": "str (optional)",
                        "source": "str (optional)"
                    }
                ],
                "id": "int",
                "jobType": "str (optional)",
                "livyInfo": {
                    "currentState": "str (optional)",
                    "deadAt": "datetime (optional)",
                    "jobCreationRequest": {
                        "archives": [
                            "str (optional)"
                        ],
                        "args": [
                            "str (optional)"
                        ],
                        "className": "str (optional)",
                        "conf": {
                            "str": "str (optional)"
                        },
                        "driverCores": "int (optional)",
                        "driverMemory": "str (optional)",
                        "executorCores": "int (optional)",
                        "executorMemory": "str (optional)",
                        "file": "str (optional)",
                        "files": [
                            "str (optional)"
                        ],
                        "jars": [
                            "str (optional)"
                        ],
                        "name": "str (optional)",
                        "numExecutors": "int (optional)",
                        "pyFiles": [
                            "str (optional)"
                        ]
                    },
                    "killedAt": "datetime (optional)",
                    "notStartedAt": "datetime (optional)",
                    "recoveringAt": "datetime (optional)",
                    "runningAt": "datetime (optional)",
                    "startingAt": "datetime (optional)",
                    "successAt": "datetime (optional)"
                },
                "log": [
                    "str (optional)"
                ],
                "name": "str (optional)",
                "pluginInfo": {
                    "cleanupStartedAt": "datetime (optional)",
                    "currentState": "str (optional)",
                    "monitoringStartedAt": "datetime (optional)",
                    "preparationStartedAt": "datetime (optional)",
                    "resourceAcquisitionStartedAt": "datetime (optional)",
                    "submissionStartedAt": "datetime (optional)"
                },
                "result": "str (optional)",
                "schedulerInfo": {
                    "cancellationRequestedAt": "datetime (optional)",
                    "currentState": "str (optional)",
                    "endedAt": "datetime (optional)",
                    "scheduledAt": "datetime (optional)",
                    "submittedAt": "datetime (optional)"
                },
                "sparkPoolName": "str (optional)",
                "state": "str (optional)",
                "submitterId": "str (optional)",
                "submitterName": "str (optional)",
                "tags": {
                    "str": "str (optional)"
                },
                "workspaceName": "str (optional)"
            }
    """

    content_type = kwargs.pop('content_type', None)  # type: Optional[str]
    detailed = kwargs.pop('detailed', None)  # type: Optional[bool]

    accept = "application/json"

    # Construct URL
    url = kwargs.pop("template_url", '/batches')

    # Construct parameters
    query_parameters = kwargs.pop("params", {})  # type: Dict[str, Any]
    if detailed is not None:
        query_parameters['detailed'] = _SERIALIZER.query("detailed", detailed, 'bool')

    # Construct headers
    header_parameters = kwargs.pop("headers", {})  # type: Dict[str, Any]
    if content_type is not None:
        header_parameters['Content-Type'] = _SERIALIZER.header("content_type", content_type, 'str')
    header_parameters['Accept'] = _SERIALIZER.header("accept", accept, 'str')

    return HttpRequest(
        method="POST",
        url=url,
        params=query_parameters,
        headers=header_parameters,
        **kwargs
    )


def build_get_spark_batch_job_request(
    batch_id,  # type: int
    **kwargs  # type: Any
):
    # type: (...) -> HttpRequest
    """Gets a single spark batch job.

    See https://aka.ms/azsdk/python/llcwiki for how to incorporate this request builder into your
    code flow.

    :param batch_id: Identifier for the batch job.
    :type batch_id: int
    :keyword detailed: Optional query param specifying whether detailed response is returned beyond
     plain livy.
    :paramtype detailed: bool
    :return: Returns an :class:`~azure.synapse.spark.core.rest.HttpRequest` that you will pass to
     the client's `send_request` method. See https://aka.ms/azsdk/python/protocol/quickstart for how
     to incorporate this response into your code flow.
    :rtype: ~azure.synapse.spark.core.rest.HttpRequest

    Example:
        .. code-block:: python

            # response body for status code(s): 200
            response.json() == {
                "appId": "str (optional)",
                "appInfo": {
                    "str": "str (optional)"
                },
                "artifactId": "str (optional)",
                "errorInfo": [
                    {
                        "errorCode": "str (optional)",
                        "message": "str (optional)",
                        "source": "str (optional)"
                    }
                ],
                "id": "int",
                "jobType": "str (optional)",
                "livyInfo": {
                    "currentState": "str (optional)",
                    "deadAt": "datetime (optional)",
                    "jobCreationRequest": {
                        "archives": [
                            "str (optional)"
                        ],
                        "args": [
                            "str (optional)"
                        ],
                        "className": "str (optional)",
                        "conf": {
                            "str": "str (optional)"
                        },
                        "driverCores": "int (optional)",
                        "driverMemory": "str (optional)",
                        "executorCores": "int (optional)",
                        "executorMemory": "str (optional)",
                        "file": "str (optional)",
                        "files": [
                            "str (optional)"
                        ],
                        "jars": [
                            "str (optional)"
                        ],
                        "name": "str (optional)",
                        "numExecutors": "int (optional)",
                        "pyFiles": [
                            "str (optional)"
                        ]
                    },
                    "killedAt": "datetime (optional)",
                    "notStartedAt": "datetime (optional)",
                    "recoveringAt": "datetime (optional)",
                    "runningAt": "datetime (optional)",
                    "startingAt": "datetime (optional)",
                    "successAt": "datetime (optional)"
                },
                "log": [
                    "str (optional)"
                ],
                "name": "str (optional)",
                "pluginInfo": {
                    "cleanupStartedAt": "datetime (optional)",
                    "currentState": "str (optional)",
                    "monitoringStartedAt": "datetime (optional)",
                    "preparationStartedAt": "datetime (optional)",
                    "resourceAcquisitionStartedAt": "datetime (optional)",
                    "submissionStartedAt": "datetime (optional)"
                },
                "result": "str (optional)",
                "schedulerInfo": {
                    "cancellationRequestedAt": "datetime (optional)",
                    "currentState": "str (optional)",
                    "endedAt": "datetime (optional)",
                    "scheduledAt": "datetime (optional)",
                    "submittedAt": "datetime (optional)"
                },
                "sparkPoolName": "str (optional)",
                "state": "str (optional)",
                "submitterId": "str (optional)",
                "submitterName": "str (optional)",
                "tags": {
                    "str": "str (optional)"
                },
                "workspaceName": "str (optional)"
            }
    """

    detailed = kwargs.pop('detailed', None)  # type: Optional[bool]

    accept = "application/json"

    # Construct URL
    url = kwargs.pop("template_url", '/batches/{batchId}')
    path_format_arguments = {
        'batchId': _SERIALIZER.url("batch_id", batch_id, 'int'),
    }
    url = _format_url_section(url, **path_format_arguments)

    # Construct parameters
    query_parameters = kwargs.pop("params", {})  # type: Dict[str, Any]
    if detailed is not None:
        query_parameters['detailed'] = _SERIALIZER.query("detailed", detailed, 'bool')

    # Construct headers
    header_parameters = kwargs.pop("headers", {})  # type: Dict[str, Any]
    header_parameters['Accept'] = _SERIALIZER.header("accept", accept, 'str')

    return HttpRequest(
        method="GET",
        url=url,
        params=query_parameters,
        headers=header_parameters,
        **kwargs
    )


def build_cancel_spark_batch_job_request(
    batch_id,  # type: int
    **kwargs  # type: Any
):
    # type: (...) -> HttpRequest
    """Cancels a running spark batch job.

    See https://aka.ms/azsdk/python/llcwiki for how to incorporate this request builder into your
    code flow.

    :param batch_id: Identifier for the batch job.
    :type batch_id: int
    :return: Returns an :class:`~azure.synapse.spark.core.rest.HttpRequest` that you will pass to
     the client's `send_request` method. See https://aka.ms/azsdk/python/protocol/quickstart for how
     to incorporate this response into your code flow.
    :rtype: ~azure.synapse.spark.core.rest.HttpRequest
    """



    # Construct URL
    url = kwargs.pop("template_url", '/batches/{batchId}')
    path_format_arguments = {
        'batchId': _SERIALIZER.url("batch_id", batch_id, 'int'),
    }
    url = _format_url_section(url, **path_format_arguments)

    return HttpRequest(
        method="DELETE",
        url=url,
        **kwargs
    )

